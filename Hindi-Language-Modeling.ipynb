{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hindi2Vec Language Modeling\n",
    "\n",
    "The goal of this notebook is to train Hindi word embeddings using the [fast.ai](http://www.fast.ai/) version of [AWD LSTM Language Model](https://arxiv.org/abs/1708.02182)--basically LSTM with dropouts--with data from [Wikipedia](https://dumps.wikimedia.org/hiwiki/latest/hiwiki-latest-pages-articles.xml.bz2) (pulled on March 2, 2018). \n",
    "\n",
    "We achieved perplexity of XX with YY embeddings, compared to [state-of-the-art on November 17, 2017](https://github.com/RedditSota/state-of-the-art-result-for-machine-learning-problems) at 40.68 for English language. To the best of our knowledge, there is no comparable research in Hindi language at the point of writing (March 2, 2018)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import dill as pickle\n",
    "import json\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "from spacy.lang.hi import *\n",
    "\n",
    "import re\n",
    "import torchtext\n",
    "from torchtext import vocab, data\n",
    "from torchtext.datasets import language_modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.learner import *\n",
    "from fastai.rnn_reg import *\n",
    "from fastai.rnn_train import *\n",
    "from fastai.nlp import *\n",
    "from fastai.lm_rnn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-25T03:30:25.407566Z",
     "start_time": "2018-01-25T03:30:21.597641Z"
    }
   },
   "outputs": [],
   "source": [
    "PATH='C:\\\\Users\\\\nirantk\\\\Desktop\\\\hindi-wiki\\\\'\n",
    "# path to the xml wikipedia dump\n",
    "\n",
    "EXT_PATH = 'extract\\\\'\n",
    "TRN_PATH = 'train\\\\'\n",
    "VAL_PATH = 'valid\\\\'\n",
    "SAMPLE_PATH = 'sample\\\\'\n",
    "\n",
    "EXT = f'{PATH}{EXT_PATH}'\n",
    "TRN = f'{PATH}{TRN_PATH}'\n",
    "VAL = f'{PATH}{VAL_PATH}'\n",
    "SAMPLE = f'{PATH}{SAMPLE_PATH}'\n",
    "\n",
    "ext_files = !ls {EXT}\n",
    "sample_files = !ls {SAMPLE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T10:16:59.532902Z",
     "start_time": "2018-01-16T10:16:59.460836Z"
    }
   },
   "outputs": [],
   "source": [
    "# Exact command line statement used to extract content from xml file\n",
    "# !python wikiextractor/WikiExtractor.py hiwiki-latest-pages-articles.xml -o extract -b 10M --ignored_tags abbr,b,big --discard_elements gallery,timeline,noinclude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save as Plain Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Wall time: 8.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%prun\n",
    "\n",
    "\n",
    "def clean_files(extracted_filelist, TRN):    \n",
    "    cleaned_all = []\n",
    "    for ext_file in extracted_filelist:\n",
    "        input_file = f'{EXT}{ext_file}'\n",
    "        with open(input_file,'r', encoding='utf-8') as f:\n",
    "            raw_txt = f.readlines()\n",
    "            cleaned_doc = []\n",
    "            for line in raw_txt:\n",
    "                new_line = re.sub('<[^<]+?>', '', line)\n",
    "                new_line = re.sub('__[^<]+?__', '', new_line) \n",
    "                new_line = new_line.strip()\n",
    "                if new_line != '':\n",
    "                    cleaned_doc.append(new_line)\n",
    "\n",
    "            new_doc = \"\\n\".join(cleaned_doc)\n",
    "            cleaned_all.append(new_doc)\n",
    "            with open(f\"{TRN}{ext_file}.txt\", \"w\", encoding='utf-8') as text_file:\n",
    "                text_file.write(new_doc)\n",
    "    return cleaned_all\n",
    "\n",
    "cleaned_all = clean_files(ext_files, TRN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview:\n",
      "हम होंगे कामयाब\n",
      "दैनिक पूजा\n",
      "दैनिक पूजा विधि हिन्दू धर्म की कई उपासना पद्धतियों में से एक है। ये एक दैनिक कर्म है। विभिन्न देवताओं को प्रसन्न करने के लिये कई मन्त्र बताये गये हैं, जो लगभग सभी पुराणों से हैं। वैदिक मन्त्र यज्ञ और हवन के लिये होते हैं।\n",
      "पूजा की रीति इस तरह है : पहले कोई भी देवता चुनें, जिसकी पूजा करनी है। फ़िर विधिवत निम्नलिखित मन्त्रों (सभी संस्कृत में हैं) के साथ उसकी पूजा करें। पौराणिक देवताओं के मन्त्र इस प्रकार हैं :\n",
      "नीचे लिखे मन्त्र गणेश के लिये हैं :\n",
      "शुक्लाम्बर धरं विष्णुं शशि\n",
      "\n",
      "Length of list: 37\n"
     ]
    }
   ],
   "source": [
    "print(f'Preview:\\n{cleaned_all[0][:500]}\\n\\nLength of list (should be equal to number of documents): {len(cleaned_all)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T15:19:28.662393Z",
     "start_time": "2018-01-17T15:19:28.603161Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(trn_files)\n",
    "len_valid = int(0.2 * len(trn_files)) \n",
    "val_files = trn_files[:len_valid]\n",
    "trn_files = trn_files[len_valid:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T15:19:34.451766Z",
     "start_time": "2018-01-17T15:19:34.392211Z"
    }
   },
   "outputs": [],
   "source": [
    "# # WARNING: Run exactly ONCE\n",
    "\n",
    "# import shutil, os\n",
    "# for root, dirs, files in os.walk(TRN):\n",
    "#     for file in files:\n",
    "#         if file.endswith(\".txt\") & (file in val_files):\n",
    "#             shutil.move(os.path.join(root, file),VAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T15:20:30.038345Z",
     "start_time": "2018-01-17T15:20:29.934723Z"
    }
   },
   "outputs": [],
   "source": [
    "trn_files = !ls {TRN}\n",
    "val_files = !ls {VAL}\n",
    "print(trn_files), print(val_files), print(len(trn_files)), print(len(val_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize(document):\n",
    "    nlp = Hindi() # from spacy.lang.hi\n",
    "    return [token.text for token in nlp(document)]\n",
    "    \n",
    "def docs_tokenize(documents_as_lists):   \n",
    "    for document in documents_as_lists:\n",
    "        tokens = word_tokenize(document)\n",
    "        tokens_list.extend(tokens)\n",
    "    \n",
    "    return tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from tokens_list.txt\n",
      "Found 30598557 tokens\n",
      "Wall time: 15.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokens_filename = \"tokens_list.txt\"\n",
    "tokens_list = []\n",
    "\n",
    "#TODO refactor from try except blocks to if else with if statement checking if file exists using Pathlib\n",
    "\n",
    "try:\n",
    "    print(f'Reading from {tokens_filename}')\n",
    "    with open(tokens_filename, \"r\") as f:\n",
    "         tokens_list = json.load(f)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f'FileNotFound. Trying to tokenize from cleaned_all')\n",
    "    tokens_list = docs_tokenize(cleaned_all)\n",
    "    \n",
    "    with open('tokens_list.txt', 'w') as outfile:\n",
    "        json.dump(tokens_list, outfile)\n",
    "\n",
    "print(f'Found {len(tokens_list)} tokens') # 30598557 tokens expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-24T15:51:18.836570Z",
     "start_time": "2018-01-24T15:51:18.535781Z"
    }
   },
   "outputs": [],
   "source": [
    "# TEXT = pickle.load(open(f'{PATH}models/TEXT.pkl','rb'))\n",
    "# freqs = pd.DataFrame.from_dict(TEXT.vocab.freqs,orient='index')\n",
    "# freqs.sort_values(0,ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-24T15:51:57.284742Z",
     "start_time": "2018-01-24T15:51:57.143426Z"
    }
   },
   "outputs": [],
   "source": [
    "# cnt = []\n",
    "# for i in range(49):\n",
    "#     row_cnt = freqs[freqs[0]>=i+1].shape[0]\n",
    "#     cnt.append(row_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-24T15:51:58.695888Z",
     "start_time": "2018-01-24T15:51:58.523805Z"
    }
   },
   "outputs": [],
   "source": [
    "# plt.plot(cnt)\n",
    "# plt.axvline(x=10,color='red', linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-24T15:52:42.170567Z",
     "start_time": "2018-01-24T15:52:42.118686Z"
    }
   },
   "outputs": [],
   "source": [
    "# #number of legitimate thai words at freqs\n",
    "# freqs[freqs[0]<=10].sort_values(ascending=False,by=0).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-24T16:04:14.088043Z",
     "start_time": "2018-01-24T16:04:14.060994Z"
    }
   },
   "outputs": [],
   "source": [
    "TEXT = data.Field(lower=True, tokenize=word_tokenize)\n",
    "#batch size\n",
    "bs=64\n",
    "#backprop through time\n",
    "bptt=70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-24T16:41:27.289827Z",
     "start_time": "2018-01-24T16:04:14.892151Z"
    }
   },
   "outputs": [],
   "source": [
    "#FILES = dict(train=f'{SAMPLE_PATH}', validation=f'{SAMPLE_PATH}', test=f'{SAMPLE_PATH}')\n",
    "FILES = dict(train=f'{TRN_PATH}', validation=f'{VAL_PATH}', test=f'{SAMPLE_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "md = LanguageModelData.from_text_files(PATH, TEXT, **FILES, bs=bs, bptt=bptt, min_freq=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(TEXT, open(f'{PATH}models/TEXT.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-24T16:41:27.379231Z",
     "start_time": "2018-01-24T16:41:27.336150Z"
    }
   },
   "outputs": [],
   "source": [
    "#trn_ds is list; one for each txt file\n",
    "txt = md.trn_ds[0].text[:10]\n",
    "TEXT.numericalize([txt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-24T16:41:27.406405Z",
     "start_time": "2018-01-24T16:41:27.380815Z"
    }
   },
   "outputs": [],
   "source": [
    "em_sz = 300  # size of each embedding vector\n",
    "nh = 500     # number of hidden activations per layer\n",
    "nl = 3       # number of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-24T16:41:27.429580Z",
     "start_time": "2018-01-24T16:41:27.407646Z"
    }
   },
   "outputs": [],
   "source": [
    "opt_fn = partial(optim.Adam, betas=(0.7, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-24T16:41:27.669131Z",
     "start_time": "2018-01-24T16:41:27.431195Z"
    }
   },
   "outputs": [],
   "source": [
    "learner = md.get_model(opt_fn, em_sz, nh, nl,\n",
    "               dropouti=0.05, dropout=0.05, wdrop=0.1, dropoute=0.02, dropouth=0.05)\n",
    "learner.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\n",
    "learner.clip=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-24T16:42:11.341458Z",
     "start_time": "2018-01-24T16:41:27.701583Z"
    }
   },
   "outputs": [],
   "source": [
    "#find suitable learning rates\n",
    "learner.lr_find(1e-07,1e2)\n",
    "learner.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-24T16:57:14.166255Z",
     "start_time": "2018-01-24T16:56:54.144823Z"
    }
   },
   "outputs": [],
   "source": [
    "#loss 3.85956; perplexity 47.44\n",
    "learner.fit(1e-3, 4, wds=1e-6, cycle_len=1, cycle_mult=2)\n",
    "#learner.save_encoder('adam1_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-24T17:03:43.526685Z",
     "start_time": "2018-01-24T17:03:43.486167Z"
    }
   },
   "outputs": [],
   "source": [
    "Image(filename= \"/home/ubuntu/data/thaiwiki/png/adam1_enc.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-26T04:31:17.340615Z",
     "start_time": "2017-12-26T04:31:04.722284Z"
    }
   },
   "outputs": [],
   "source": [
    "#loss 3.84171; perplexity 46.61\n",
    "learner.fit(1e-3, 1, wds=1e-6, cycle_len=20, cycle_save_name='adam3_20')\n",
    "#learner.save_encoder('adam3_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-24T17:01:21.317894Z",
     "start_time": "2018-01-24T17:01:21.283095Z"
    }
   },
   "outputs": [],
   "source": [
    "Image(filename= \"/home/ubuntu/data/thaiwiki/png/adam3_enc.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-24T17:09:04.381055Z",
     "start_time": "2018-01-24T17:05:17.677117Z"
    }
   },
   "outputs": [],
   "source": [
    "#m = learner.model\n",
    "#pickle.dump(m,open(f'{PATH}models/wiki_lang.pkl','wb'))\n",
    "TEXT = pickle.load(open(f'{PATH}models/TEXT.pkl','rb'))\n",
    "m = pickle.load(open(f'{PATH}models/wiki_lang.pkl','rb'))\n",
    "m[0].bs=1\n",
    "m.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-24T17:13:05.644224Z",
     "start_time": "2018-01-24T17:13:05.588131Z"
    }
   },
   "outputs": [],
   "source": [
    "def gen_text(ss,topk):\n",
    "    s = [word_tokenize(ss)]\n",
    "    t = TEXT.numericalize(s)\n",
    "    m.reset()\n",
    "    pred,*_ = m(t)\n",
    "    pred_i = torch.topk(pred[-1], topk)[1]\n",
    "    return [TEXT.vocab.itos[o] for o in to_np(pred_i)]\n",
    "\n",
    "def gen_sentences(ss,nb_words):\n",
    "    result = []\n",
    "    s = [word_tokenize(ss)]\n",
    "    t = TEXT.numericalize(s)\n",
    "    m.reset()\n",
    "    pred,*_ = m(t)\n",
    "    for i in range(nb_words):\n",
    "        pred_i = pred[-1].topk(2)[1]\n",
    "        pred_i = pred_i[1] if pred_i.data[0] < 2 else pred_i[0]\n",
    "        result.append(TEXT.vocab.itos[pred_i.data[0]])\n",
    "        pred,*_ = m(pred_i[0].unsqueeze(0))\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-24T17:13:36.841186Z",
     "start_time": "2018-01-24T17:13:36.725232Z"
    }
   },
   "outputs": [],
   "source": [
    "ss=\"\"\"สวัสดีครับพี่น้องเสื้อ\"\"\"\n",
    "gen_text(ss,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-24T17:13:38.363900Z",
     "start_time": "2018-01-24T17:13:37.986379Z"
    }
   },
   "outputs": [],
   "source": [
    "''.join(gen_sentences(ss,50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-24T17:14:19.257675Z",
     "start_time": "2018-01-24T17:14:19.219043Z"
    }
   },
   "outputs": [],
   "source": [
    "emb_weights = list(learner.model.named_parameters())[0][1]\n",
    "emb_np = to_np(emb_weights.data)\n",
    "\n",
    "TEXT = pickle.load(open(f'{PATH}models/TEXT.pkl','rb'))\n",
    "TEXT.vocab.set_vectors(vectors=emb_weights.data,dim=300,stoi=TEXT.vocab.stoi)\n",
    "#pickle.dump(TEXT, open(f'{PATH}models/TEXT_vec.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-25T03:54:53.365410Z",
     "start_time": "2018-01-25T03:54:52.039915Z"
    }
   },
   "outputs": [],
   "source": [
    "TEXT_vec = pickle.load(open(f'{PATH}models/TEXT_vec.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-25T03:55:01.593405Z",
     "start_time": "2018-01-25T03:55:01.533652Z"
    }
   },
   "outputs": [],
   "source": [
    "thai2vec = pd.DataFrame(to_np(TEXT_vec.vocab.vectors))\n",
    "thai2vec.index = TEXT_vec.vocab.itos\n",
    "thai2vec.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-25T03:55:23.616554Z",
     "start_time": "2018-01-25T03:55:03.872457Z"
    }
   },
   "outputs": [],
   "source": [
    "thai2save = thai2vec[~thai2vec.index.str.contains(' ')]\n",
    "#remove lines with weird characters due to bad segmentation\n",
    "thai2save = thai2save.iloc[3:-48,:]\n",
    "#thai2save.to_csv(f'{PATH}models/thai2vec.vec',sep=' ',header=False, line_terminator='\\n')\n",
    "#add NB_ROWS NB_COLS as header\n",
    "thai2save.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-25T03:59:08.601229Z",
     "start_time": "2018-01-25T03:59:01.839159Z"
    }
   },
   "outputs": [],
   "source": [
    "#model.save_word2vec_format(f'{PATH}models/thai2vec.vec',f'{PATH}models/thai2vec.vocab',False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastAI",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
